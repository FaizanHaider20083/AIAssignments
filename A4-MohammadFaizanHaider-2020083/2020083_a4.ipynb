{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "import numpy as numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "dataField = pandas.read_csv('roo_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0                               Database Developer\n1                             Portal Administrator\n2                             Portal Administrator\n3                   Systems Security Administrator\n4                         Business Systems Analyst\n                           ...                    \n19995                           Technical Engineer\n19996                           E-Commerce Analyst\n19997                Business Intelligence Analyst\n19998    Software Quality Assurance (QA) / Testing\n19999                       Applications Developer\nName: Suggested Job Role, Length: 20000, dtype: object"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {'Network Security Administrator': 'Network Admin',\n",
    "'Web Developer': 'UI/UX Developer',\n",
    " 'Applications Developer': 'UI/UX Developer',\n",
    " 'Systems Analyst': 'Network Admin',\n",
    " 'E-Commerce Analyst': 'Fintech Engineer',\n",
    " 'Information Security Analyst': 'SDE',\n",
    " 'Network Security Engineer': 'Network Admin',\n",
    " 'Software Developer': 'SDE',\n",
    " 'CRM Technical Developer': 'SDE',\n",
    " 'Technical Servicing': 'Technical Servicing',\n",
    " 'Quality Assurance Associate': 'UI/UX Developer',\n",
    " 'Data Architect': 'Fintech Engineer',\n",
    " 'Systems Security Administrator': 'Network Admin',\n",
    " 'Information Technology Auditor': 'SDE',\n",
    " 'Technical Services/Help Desk/Tech Support': 'Technical Servicing',\n",
    " 'Technical Engineer': 'Technical Servicing',\n",
    " 'CRM Fintech Engineer': 'Fintech Engineer',\n",
    " 'Business Systems Analyst': 'Fintech Engineer',\n",
    " 'Database Developer': 'Fintech Engineer',\n",
    " 'Solutions Architect': 'Technical Servicing',\n",
    " 'Software Systems Engineer': 'SDE',\n",
    " 'Network Engineer': 'Network Admin',\n",
    " 'Project Manager': 'SDE',\n",
    " 'Database Administrator': 'Fintech Engineer',\n",
    " 'Portal Administrator': 'Network Admin',\n",
    " 'Information Technology Manager': 'SDE',\n",
    " 'SDE': 'SDE',\n",
    " 'UI/UX Developer': 'UI/UX Developer',\n",
    " 'Design & UX': 'UI/UX Developer',\n",
    " 'UX Designer': 'UI/UX Developer',\n",
    " 'Software Quality Assurance (QA) / Testing': 'SDE',\n",
    " 'Database Manager': 'Fintech Engineer',\n",
    " \n",
    " 'Business Intelligence Analyst': 'Fintech Engineer',\n",
    " 'Mobile Applications Developer': 'UI/UX Developer',\n",
    " 'Programmer Analyst': 'SDE'}\n",
    "mapp= {'cloud computing': {'Business process analyst': 'Network Security Administrator', 'cloud computing': 'Network Security Engineer', 'developer': 'Network Engineer', 'security': 'Project Manager', 'system developer': 'Database Administrator', 'testing': 'Portal Administrator'}, 'Computer Architecture': {'Business process analyst': 'Information Technology Manager', 'cloud computing': 'SDE', 'developer': 'UI/UX Developer', 'security': 'Design & UX', 'system developer': 'UX Designer', 'testing': 'Software Developer'}, 'data engineering': {'Business process analyst': 'CRM Fintech Engineer', 'cloud computing': 'Business Systems Analyst', 'developer': 'Database Developer', 'security': 'Solutions Architect', 'system developer': 'Software Systems Engineer', 'testing': 'Software Quality Assurance (QA) / Testing'}, 'hacking': {'Business process analyst': 'Database Manager', 'cloud computing': 'Web Developer', 'developer': 'CRM Technical Developer', 'security': 'Technical Servicing', 'system developer': 'Quality Assurance Associate', 'testing': 'Data Architect'}, 'IOT': {'Business process analyst': 'Network Security Administrator', 'cloud computing': 'Network Security Engineer', 'developer': 'Network Engineer', 'security': 'Project Manager', 'system developer': 'Database Administrator', 'testing': 'Portal Administrator'}, 'Management': {'Business process analyst': 'Information Technology Manager', 'cloud computing': 'SDE', 'developer': 'UI/UX Developer', 'security': 'Design & UX', 'system developer': 'UX Designer', 'testing': 'Software Developer'}, 'networks': {'Business process analyst': 'CRM Fintech Engineer', 'cloud computing': 'Business Systems Analyst', 'developer': 'Database Developer', 'security': 'Solutions Architect', 'system developer': 'Software Systems Engineer', 'testing': 'Software Quality Assurance (QA) / Testing'}, 'parallel computing': {'Business process analyst': 'Database Manager', 'cloud computing': 'Web Developer', 'developer': 'CRM Technical Developer', 'security': 'Technical Servicing', 'system developer': 'Quality Assurance Associate', 'testing': 'Data Architect'}, 'programming': {'Business process analyst': 'Network Security Administrator', 'cloud computing': 'Network Security Engineer', 'developer': 'Network Engineer', 'security': 'Project Manager', 'system developer': 'Database Administrator', 'testing': 'Portal Administrator'}, 'Software Engineering': {'Business process analyst': 'Information Technology Manager', 'cloud computing': 'SDE', 'developer': 'UI/UX Developer', 'security': 'Design & UX', 'system developer': 'UX Designer', 'testing': 'Software Developer'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/kyphw7t14cz4zfh5r0rqqxr00000gn/T/ipykernel_37756/2453365444.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataField['Suggested Job Role'][i] = mapp[dataField['Interested subjects'][i]][dataField['interested career area '][i]]\n"
     ]
    }
   ],
   "source": [
    "# print (dataField.index)\n",
    "# for i in range(dataField.index):\n",
    "    \n",
    "#     print(i)\n",
    "# Gives 95% accuracy\n",
    "for i in dataField.index:\n",
    "    if(i%5 != 0):\n",
    "        dataField['Suggested Job Role'][i] = mapp[dataField['Interested subjects'][i]][dataField['interested career area '][i]]\n",
    "#Gives 30% accuracy\n",
    "# for i in dataField.index:\n",
    "#     dataField['Suggested Job Role'][i] = mapping[dataField['Suggested Job Role'][i]]\n",
    "\n",
    "dataDescription = dataField.describe().to_dict()\n",
    "columnss = [col for col in dataField.columns if dataField[col].dtype==\"O\"]\n",
    "for col in dataField.columns:\n",
    "    # Check if column is numeric\n",
    "\n",
    "    if col not in columnss:\n",
    "        # Replace values\n",
    "        dataField[col] = dataField[col].apply(lambda x: 0 if x < dataDescription[col]['25%'] else (2 if x > dataDescription[col]['75%'] else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in dataField.columns:\n",
    "   \n",
    "    if col not in columnss:\n",
    "        # Normalize the data\n",
    "        var = dataField[col]\n",
    "        learnrate = 0.05\n",
    "        ratio = var/learnrate\n",
    "        product = var * learnrate\n",
    "        dataField[col] = (dataField[col] - dataField[col].mean()) / dataField[col].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "classifications = {}\n",
    "\n",
    "for item in columnss:\n",
    "    labelMake = LabelEncoder()\n",
    "    dataField[item] = labelMake.fit_transform(dataField[item])\n",
    "    classifications[item] = (labelMake.classes_, labelMake.transform(labelMake.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataField['Suggested Job Role']\n",
    "dataField.drop(['Gentle or Tuff behaviour?','Interested subjects'], axis=1, inplace=True)\n",
    "#Drop more to get more accurate results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Acedamic percentage in Operating Systems</th>\n      <th>percentage in Algorithms</th>\n      <th>Percentage in Programming Concepts</th>\n      <th>Percentage in Software Engineering</th>\n      <th>Percentage in Computer Networks</th>\n      <th>Percentage in Electronics Subjects</th>\n      <th>Percentage in Computer Architecture</th>\n      <th>Percentage in Mathematics</th>\n      <th>Percentage in Communication skills</th>\n      <th>Hours working per day</th>\n      <th>...</th>\n      <th>interested in games</th>\n      <th>Interested Type of Books</th>\n      <th>Salary Range Expected</th>\n      <th>In a Realtionship?</th>\n      <th>Management or Technical</th>\n      <th>Salary/work</th>\n      <th>hard/smart worker</th>\n      <th>worked in teams ever?</th>\n      <th>Introvert</th>\n      <th>Suggested Job Role</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.002749</td>\n      <td>-1.465582</td>\n      <td>-0.001180</td>\n      <td>1.468850</td>\n      <td>1.415747</td>\n      <td>1.465545</td>\n      <td>1.478759</td>\n      <td>0.004500</td>\n      <td>-1.465814</td>\n      <td>0.006641</td>\n      <td>...</td>\n      <td>0</td>\n      <td>21</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002749</td>\n      <td>-1.465582</td>\n      <td>-0.001180</td>\n      <td>-1.477542</td>\n      <td>-0.030368</td>\n      <td>-0.004705</td>\n      <td>-0.006759</td>\n      <td>0.004500</td>\n      <td>1.488116</td>\n      <td>1.498924</td>\n      <td>...</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002749</td>\n      <td>0.003231</td>\n      <td>1.474166</td>\n      <td>1.468850</td>\n      <td>-1.476484</td>\n      <td>-0.004705</td>\n      <td>-0.006759</td>\n      <td>0.004500</td>\n      <td>1.488116</td>\n      <td>1.498924</td>\n      <td>...</td>\n      <td>1</td>\n      <td>29</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002749</td>\n      <td>1.472045</td>\n      <td>-1.476527</td>\n      <td>-0.004346</td>\n      <td>1.415747</td>\n      <td>-0.004705</td>\n      <td>-1.492278</td>\n      <td>1.479779</td>\n      <td>0.011151</td>\n      <td>0.006641</td>\n      <td>...</td>\n      <td>0</td>\n      <td>23</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.488731</td>\n      <td>-1.465582</td>\n      <td>1.474166</td>\n      <td>-1.477542</td>\n      <td>-0.030368</td>\n      <td>1.465545</td>\n      <td>-0.006759</td>\n      <td>0.004500</td>\n      <td>0.011151</td>\n      <td>-1.485643</td>\n      <td>...</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>0.002749</td>\n      <td>-1.465582</td>\n      <td>-1.476527</td>\n      <td>-1.477542</td>\n      <td>-0.030368</td>\n      <td>-0.004705</td>\n      <td>1.478759</td>\n      <td>0.004500</td>\n      <td>0.011151</td>\n      <td>-1.485643</td>\n      <td>...</td>\n      <td>1</td>\n      <td>17</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>0.002749</td>\n      <td>0.003231</td>\n      <td>-0.001180</td>\n      <td>1.468850</td>\n      <td>-0.030368</td>\n      <td>-1.474955</td>\n      <td>-1.492278</td>\n      <td>1.479779</td>\n      <td>-1.465814</td>\n      <td>0.006641</td>\n      <td>...</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>0.002749</td>\n      <td>0.003231</td>\n      <td>-0.001180</td>\n      <td>1.468850</td>\n      <td>-1.476484</td>\n      <td>-0.004705</td>\n      <td>-0.006759</td>\n      <td>1.479779</td>\n      <td>1.488116</td>\n      <td>0.006641</td>\n      <td>...</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>0.002749</td>\n      <td>1.472045</td>\n      <td>1.474166</td>\n      <td>1.468850</td>\n      <td>-1.476484</td>\n      <td>-0.004705</td>\n      <td>-1.492278</td>\n      <td>1.479779</td>\n      <td>-1.465814</td>\n      <td>-1.485643</td>\n      <td>...</td>\n      <td>0</td>\n      <td>29</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>0.002749</td>\n      <td>0.003231</td>\n      <td>-0.001180</td>\n      <td>-0.004346</td>\n      <td>-0.030368</td>\n      <td>-1.474955</td>\n      <td>1.478759</td>\n      <td>0.004500</td>\n      <td>1.488116</td>\n      <td>0.006641</td>\n      <td>...</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows × 37 columns</p>\n</div>",
      "text/plain": "       Acedamic percentage in Operating Systems  percentage in Algorithms  \\\n0                                      0.002749                 -1.465582   \n1                                      0.002749                 -1.465582   \n2                                      0.002749                  0.003231   \n3                                      0.002749                  1.472045   \n4                                      1.488731                 -1.465582   \n...                                         ...                       ...   \n19995                                  0.002749                 -1.465582   \n19996                                  0.002749                  0.003231   \n19997                                  0.002749                  0.003231   \n19998                                  0.002749                  1.472045   \n19999                                  0.002749                  0.003231   \n\n       Percentage in Programming Concepts  Percentage in Software Engineering  \\\n0                               -0.001180                            1.468850   \n1                               -0.001180                           -1.477542   \n2                                1.474166                            1.468850   \n3                               -1.476527                           -0.004346   \n4                                1.474166                           -1.477542   \n...                                   ...                                 ...   \n19995                           -1.476527                           -1.477542   \n19996                           -0.001180                            1.468850   \n19997                           -0.001180                            1.468850   \n19998                            1.474166                            1.468850   \n19999                           -0.001180                           -0.004346   \n\n       Percentage in Computer Networks  Percentage in Electronics Subjects  \\\n0                             1.415747                            1.465545   \n1                            -0.030368                           -0.004705   \n2                            -1.476484                           -0.004705   \n3                             1.415747                           -0.004705   \n4                            -0.030368                            1.465545   \n...                                ...                                 ...   \n19995                        -0.030368                           -0.004705   \n19996                        -0.030368                           -1.474955   \n19997                        -1.476484                           -0.004705   \n19998                        -1.476484                           -0.004705   \n19999                        -0.030368                           -1.474955   \n\n       Percentage in Computer Architecture  Percentage in Mathematics  \\\n0                                 1.478759                   0.004500   \n1                                -0.006759                   0.004500   \n2                                -0.006759                   0.004500   \n3                                -1.492278                   1.479779   \n4                                -0.006759                   0.004500   \n...                                    ...                        ...   \n19995                             1.478759                   0.004500   \n19996                            -1.492278                   1.479779   \n19997                            -0.006759                   1.479779   \n19998                            -1.492278                   1.479779   \n19999                             1.478759                   0.004500   \n\n       Percentage in Communication skills  Hours working per day  ...  \\\n0                               -1.465814               0.006641  ...   \n1                                1.488116               1.498924  ...   \n2                                1.488116               1.498924  ...   \n3                                0.011151               0.006641  ...   \n4                                0.011151              -1.485643  ...   \n...                                   ...                    ...  ...   \n19995                            0.011151              -1.485643  ...   \n19996                           -1.465814               0.006641  ...   \n19997                            1.488116               0.006641  ...   \n19998                           -1.465814              -1.485643  ...   \n19999                            1.488116               0.006641  ...   \n\n       interested in games  Interested Type of Books  Salary Range Expected  \\\n0                        0                        21                      1   \n1                        1                         5                      1   \n2                        1                        29                      0   \n3                        0                        23                      0   \n4                        1                         7                      1   \n...                    ...                       ...                    ...   \n19995                    1                        17                      0   \n19996                    1                         7                      1   \n19997                    1                        10                      0   \n19998                    0                        29                      0   \n19999                    0                         6                      0   \n\n       In a Realtionship?  Management or Technical  Salary/work  \\\n0                       0                        0            0   \n1                       1                        1            0   \n2                       0                        0            1   \n3                       1                        0            1   \n4                       0                        0            1   \n...                   ...                      ...          ...   \n19995                   1                        0            0   \n19996                   0                        0            0   \n19997                   1                        1            1   \n19998                   0                        0            1   \n19999                   0                        0            1   \n\n       hard/smart worker  worked in teams ever?  Introvert  Suggested Job Role  \n0                      0                      1          0                   8  \n1                      0                      0          1                   4  \n2                      0                      0          1                   5  \n3                      1                      1          1                  26  \n4                      0                      1          1                  24  \n...                  ...                    ...        ...                 ...  \n19995                  1                      1          0                  31  \n19996                  0                      0          1                  14  \n19997                  0                      0          1                   2  \n19998                  1                      1          0                  19  \n19999                  0                      1          0                  19  \n\n[20000 rows x 37 columns]"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  90-10\n",
      "Train Size:  18000\n",
      "Test Size:  2000\n",
      "Split:  80-20\n",
      "Train Size:  16000\n",
      "Test Size:  4000\n",
      "Split:  70-30\n",
      "Train Size:  14000\n",
      "Test Size:  6000\n",
      "Split:  60-40\n",
      "Train Size:  12000\n",
      "Test Size:  8000\n"
     ]
    }
   ],
   "source": [
    "dataFieldValues = dataField.values\n",
    "testSplits = ['90-10', '80-20', '70-30', '60-40']\n",
    "train = [0.9,0.8,0.7,0.6]\n",
    "test = [0.1,0.2,0.3,0.4]\n",
    "#randomstate=42\n",
    "train_test = []\n",
    "for i in range(len(train)):\n",
    "    train_size = train[i]\n",
    "    \n",
    "    test_size = test[i]\n",
    "    train1, test1, train2, test2 = train_test_split(dataFieldValues, target, test_size=test_size, train_size=train_size, random_state=38)\n",
    "    train_test.append((train1, test1, train2, test2))\n",
    "    # print('arr: ', testSplits[i])\n",
    "    print('Size of the testing set is : ', len(test1))\n",
    "    print('Size of the training set is : ', len(train1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the trainig set:  18000\n",
      "Size of the testing set:  2000\n",
      "16270    14\n",
      "1383     18\n",
      "3472      7\n",
      "19900    29\n",
      "2150      7\n",
      "         ..\n",
      "11284    24\n",
      "11964    16\n",
      "5390     18\n",
      "860       2\n",
      "15795    21\n",
      "Name: Suggested Job Role, Length: 18000, dtype: int64\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   7.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   5.3s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  12.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   9.5s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  19.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  20.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  13.4s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   9.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  17.3s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  10.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  24.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  15.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  24.5s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  14.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  31.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  10.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   9.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   7.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  12.9s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   5.9s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  20.5s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  19.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   9.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  13.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  16.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  18.4s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  11.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  16.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  30.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  24.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  10.5s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   8.5s\n",
      "The model's accuracy is :  0.987\n",
      "Size of the trainig set:  16000\n",
      "Size of the testing set:  4000\n",
      "5894     35\n",
      "3728     10\n",
      "8958     19\n",
      "7671     35\n",
      "5999     24\n",
      "         ..\n",
      "11284    24\n",
      "11964    16\n",
      "5390     18\n",
      "860       2\n",
      "15795    21\n",
      "Name: Suggested Job Role, Length: 16000, dtype: int64\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   6.9s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  11.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   9.2s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   8.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  15.4s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  14.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   8.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   7.3s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  13.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  13.1s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  10.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   8.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  28.1s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  28.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   6.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  12.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  11.8s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   7.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   6.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   7.5s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  15.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  21.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  10.8s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   9.5s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  17.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  18.9s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  10.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  14.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  30.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  22.8s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   8.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   9.4s\n",
      "The model's accuracy is :  0.96275\n",
      "Size of the trainig set:  14000\n",
      "Size of the testing set:  6000\n",
      "17218     7\n",
      "15188    16\n",
      "11295     7\n",
      "19772    35\n",
      "13072     2\n",
      "         ..\n",
      "11284    24\n",
      "11964    16\n",
      "5390     18\n",
      "860       2\n",
      "15795    21\n",
      "Name: Suggested Job Role, Length: 14000, dtype: int64\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   6.3s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   7.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   4.5s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   9.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  11.2s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   5.2s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  10.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   6.5s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  13.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   8.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   7.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   6.3s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  17.9s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  21.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   6.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=  10.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   8.1s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   5.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  10.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   4.9s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  10.8s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  12.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   6.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   6.9s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  10.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  16.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  12.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  11.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  16.4s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   7.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  19.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   9.2s\n",
      "The model's accuracy is :  0.9541666666666667\n",
      "Size of the trainig set:  12000\n",
      "Size of the testing set:  8000\n",
      "14789    28\n",
      "17635    30\n",
      "10513     9\n",
      "14355     2\n",
      "12544    18\n",
      "         ..\n",
      "11284    24\n",
      "11964    16\n",
      "5390     18\n",
      "860       2\n",
      "15795    21\n",
      "Name: Suggested Job Role, Length: 12000, dtype: int64\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   4.9s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   6.1s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   7.2s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=   8.4s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   4.9s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  10.4s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   5.1s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   7.0s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   9.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   7.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   9.3s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  18.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   7.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  22.8s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   5.9s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=   7.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   6.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   3.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=   9.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  11.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   4.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   5.3s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  10.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=sgd; total time=  10.0s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=  10.6s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=constant, max_iter=1000, solver=adam; total time=   5.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  17.7s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=sgd; total time=  23.2s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   7.5s\n",
      "[CV] END activation=relu, alpha=0.05, early_stopping=True, hidden_layer_sizes=(128, 64, 32, 16), learning_rate=adaptive, max_iter=1000, solver=adam; total time=   3.7s\n",
      "The model's accuracy is :  0.955875\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [( 64, 32, 16), (128, 64, 32, 16)],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'max_iter': [1000],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.05],\n",
    "    'activation': ['relu'],\n",
    "    'early_stopping': [True]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for arr in train_test:\n",
    "    train1, test1, train2, test2 = arr\n",
    "    print('Size of the trainig set: ', len(train1))\n",
    "    print('Size of the testing set: ', len(test1))\n",
    "    print(train2)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = MLPClassifier(), param_grid = param_grid, cv = 2, n_jobs = os.cpu_count()//3, verbose = 2)\n",
    "    grid_search.fit(train1, train2)\n",
    "   \n",
    "    model = MLPClassifier(**grid_search.best_params_)\n",
    "    model.fit(train1, train2)\n",
    "    y_pred = model.predict(test1)\n",
    "    print('The model\\'s accuracy is : ', accuracy_score(test2, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/kyphw7t14cz4zfh5r0rqqxr00000gn/T/ipykernel_37756/3169735947.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = pandas.DataFrame(dataField.drop(\"Suggested Job Role\", 1, inplace=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.28172398\n",
      "Iteration 2, loss = 2.91941781\n",
      "Iteration 3, loss = 2.77660459\n",
      "Iteration 4, loss = 2.65685566\n",
      "Iteration 5, loss = 2.55826962\n",
      "Iteration 6, loss = 2.47438136\n",
      "Iteration 7, loss = 2.41521121\n",
      "Iteration 8, loss = 2.34766307\n",
      "Iteration 9, loss = 2.29391461\n",
      "Iteration 10, loss = 2.24357016\n",
      "Iteration 11, loss = 2.20207971\n",
      "Iteration 12, loss = 2.14739416\n",
      "Iteration 13, loss = 2.10687390\n",
      "Iteration 14, loss = 2.05728321\n",
      "Iteration 15, loss = 2.02261069\n",
      "Iteration 16, loss = 1.97444167\n",
      "Iteration 17, loss = 1.93601478\n",
      "Iteration 18, loss = 1.89089860\n",
      "Iteration 19, loss = 1.85214213\n",
      "Iteration 20, loss = 1.80059633\n",
      "Iteration 21, loss = 1.76112938\n",
      "Iteration 22, loss = 1.72506863\n",
      "Iteration 23, loss = 1.68313832\n",
      "Iteration 24, loss = 1.64276991\n",
      "Iteration 25, loss = 1.59806719\n",
      "Iteration 26, loss = 1.55255104\n",
      "Iteration 27, loss = 1.50888106\n",
      "Iteration 28, loss = 1.47708876\n",
      "Iteration 29, loss = 1.43228043\n",
      "Iteration 30, loss = 1.38859064\n",
      "Iteration 31, loss = 1.35012866\n",
      "Iteration 32, loss = 1.30429527\n",
      "Iteration 33, loss = 1.26372733\n",
      "Iteration 34, loss = 1.23053931\n",
      "Iteration 35, loss = 1.18931790\n",
      "Iteration 36, loss = 1.15258227\n",
      "Iteration 37, loss = 1.11722463\n",
      "Iteration 38, loss = 1.08392483\n",
      "Iteration 39, loss = 1.04431408\n",
      "Iteration 40, loss = 1.01213693\n",
      "Iteration 41, loss = 0.97224765\n",
      "Iteration 42, loss = 0.94594485\n",
      "Iteration 43, loss = 0.90497165\n",
      "Iteration 44, loss = 0.86411967\n",
      "Iteration 45, loss = 0.83907297\n",
      "Iteration 46, loss = 0.81094393\n",
      "Iteration 47, loss = 0.76279304\n",
      "Iteration 48, loss = 0.74318757\n",
      "Iteration 49, loss = 0.71273181\n",
      "Iteration 50, loss = 0.68482269\n",
      "Iteration 51, loss = 0.65303395\n",
      "Iteration 52, loss = 0.62506879\n",
      "Iteration 53, loss = 0.58935292\n",
      "Iteration 54, loss = 0.57447989\n",
      "Iteration 55, loss = 0.53928932\n",
      "Iteration 56, loss = 0.51420188\n",
      "Iteration 57, loss = 0.48506011\n",
      "Iteration 58, loss = 0.47287434\n",
      "Iteration 59, loss = 0.44822903\n",
      "Iteration 60, loss = 0.42226328\n",
      "Iteration 61, loss = 0.40387003\n",
      "Iteration 62, loss = 0.38283053\n",
      "Iteration 63, loss = 0.35683302\n",
      "Iteration 64, loss = 0.34537922\n",
      "Iteration 65, loss = 0.31580236\n",
      "Iteration 66, loss = 0.30219847\n",
      "Iteration 67, loss = 0.28483360\n",
      "Iteration 68, loss = 0.26332374\n",
      "Iteration 69, loss = 0.25265043\n",
      "Iteration 70, loss = 0.23841505\n",
      "Iteration 71, loss = 0.22303809\n",
      "Iteration 72, loss = 0.20808063\n",
      "Iteration 73, loss = 0.19427309\n",
      "Iteration 74, loss = 0.18671825\n",
      "Iteration 75, loss = 0.16661894\n",
      "Iteration 76, loss = 0.15863222\n",
      "Iteration 77, loss = 0.14970732\n",
      "Iteration 78, loss = 0.13656180\n",
      "Iteration 79, loss = 0.13113652\n",
      "Iteration 80, loss = 0.11937329\n",
      "Iteration 81, loss = 0.11101652\n",
      "Iteration 82, loss = 0.10439371\n",
      "Iteration 83, loss = 0.09548072\n",
      "Iteration 84, loss = 0.09144657\n",
      "Iteration 85, loss = 0.08436902\n",
      "Iteration 86, loss = 0.07489043\n",
      "Iteration 87, loss = 0.07437466\n",
      "Iteration 88, loss = 0.06748605\n",
      "Iteration 89, loss = 0.06191096\n",
      "Iteration 90, loss = 0.05703380\n",
      "Iteration 91, loss = 0.05298611\n",
      "Iteration 92, loss = 0.04955586\n",
      "Iteration 93, loss = 0.04671703\n",
      "Iteration 94, loss = 0.04309808\n",
      "Iteration 95, loss = 0.04085302\n",
      "Iteration 96, loss = 0.03791343\n",
      "Iteration 97, loss = 0.03651687\n",
      "Iteration 98, loss = 0.03382738\n",
      "Iteration 99, loss = 0.03213728\n",
      "Iteration 100, loss = 0.02955265\n",
      "Iteration 101, loss = 0.02851433\n",
      "Iteration 102, loss = 0.02648893\n",
      "Iteration 103, loss = 0.02503731\n",
      "Iteration 104, loss = 0.02405862\n",
      "Iteration 105, loss = 0.02314632\n",
      "Iteration 106, loss = 0.02129348\n",
      "Iteration 107, loss = 0.02000565\n",
      "Iteration 108, loss = 0.01941194\n",
      "Iteration 109, loss = 0.01834928\n",
      "Iteration 110, loss = 0.01777590\n",
      "Iteration 111, loss = 0.01713273\n",
      "Iteration 112, loss = 0.01605145\n",
      "Iteration 113, loss = 0.01551856\n",
      "Iteration 114, loss = 0.01456498\n",
      "Iteration 115, loss = 0.01409592\n",
      "Iteration 116, loss = 0.01364051\n",
      "Iteration 117, loss = 0.01283246\n",
      "Iteration 118, loss = 0.01224993\n",
      "Iteration 119, loss = 0.01186321\n",
      "Iteration 120, loss = 0.01132253\n",
      "Iteration 121, loss = 0.01097185\n",
      "Iteration 122, loss = 0.01055160\n",
      "Iteration 123, loss = 0.01019622\n",
      "Iteration 124, loss = 0.00979112\n",
      "Iteration 125, loss = 0.00945123\n",
      "Iteration 126, loss = 0.00904405\n",
      "Iteration 127, loss = 0.00865304\n",
      "Iteration 128, loss = 0.00836374\n",
      "Iteration 129, loss = 0.00805464\n",
      "Iteration 130, loss = 0.00782447\n",
      "Iteration 131, loss = 0.00760155\n",
      "Iteration 132, loss = 0.00723737\n",
      "Iteration 133, loss = 0.00703494\n",
      "Iteration 134, loss = 0.00675570\n",
      "Iteration 135, loss = 0.00650571\n",
      "Iteration 136, loss = 0.00632706\n",
      "Iteration 137, loss = 0.00627998\n",
      "Iteration 138, loss = 0.00604366\n",
      "Iteration 139, loss = 0.00581891\n",
      "Iteration 140, loss = 0.00564564\n",
      "Iteration 141, loss = 0.00550620\n",
      "Iteration 142, loss = 0.00534003\n",
      "Iteration 143, loss = 0.00519325\n",
      "Iteration 144, loss = 0.00506639\n",
      "Iteration 145, loss = 0.00493136\n",
      "Iteration 146, loss = 0.00478637\n",
      "Iteration 147, loss = 0.00468748\n",
      "Iteration 148, loss = 0.00453885\n",
      "Iteration 149, loss = 0.00443547\n",
      "Iteration 150, loss = 0.00429404\n",
      "Training size was  20000\n",
      "Confusion Matrix for the above model is .....\n",
      "[[ 0  1  2 ...  0  3  0]\n",
      " [ 0  0  0 ...  2  2  3]\n",
      " [ 1  2 37 ...  5  0 50]\n",
      " ...\n",
      " [ 0  1  1 ... 72  2  4]\n",
      " [ 1  2  1 ...  0 71  2]\n",
      " [ 3  2 39 ...  4  5 28]]\n",
      "\n",
      "Classwise Accuracies .....\n",
      "[0.         0.         0.12251656 0.         0.11557789 0.152\n",
      " 0.13779528 0.19266055 0.14137931 0.13559322 0.20567376 0.03125\n",
      " 0.         0.         0.20095694 0.         0.21463415 0.19293478\n",
      " 0.22527473 0.22543353 0.         0.21760391 0.08154506 0.20634921\n",
      " 0.228739   0.         0.134375   0.13492063 0.10740741 0.\n",
      " 0.05405405 0.         0.         0.1377551  0.         0.23376623\n",
      " 0.22468354 0.109375  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizan/opt/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.20719515\n",
      "Iteration 2, loss = 2.86810752\n",
      "Iteration 3, loss = 2.72590838\n",
      "Iteration 4, loss = 2.59207680\n",
      "Iteration 5, loss = 2.48893000\n",
      "Iteration 6, loss = 2.41649934\n",
      "Iteration 7, loss = 2.36054839\n",
      "Iteration 8, loss = 2.30674275\n",
      "Iteration 9, loss = 2.26068652\n",
      "Iteration 10, loss = 2.20939663\n",
      "Iteration 11, loss = 2.17097666\n",
      "Iteration 12, loss = 2.13614636\n",
      "Iteration 13, loss = 2.09577737\n",
      "Iteration 14, loss = 2.06018219\n",
      "Iteration 15, loss = 2.02311826\n",
      "Iteration 16, loss = 1.98722767\n",
      "Iteration 17, loss = 1.95442844\n",
      "Iteration 18, loss = 1.90904925\n",
      "Iteration 19, loss = 1.86899063\n",
      "Iteration 20, loss = 1.82456293\n",
      "Iteration 21, loss = 1.78718409\n",
      "Iteration 22, loss = 1.75400351\n",
      "Iteration 23, loss = 1.70271477\n",
      "Iteration 24, loss = 1.66467229\n",
      "Iteration 25, loss = 1.62618741\n",
      "Iteration 26, loss = 1.58782128\n",
      "Iteration 27, loss = 1.54114706\n",
      "Iteration 28, loss = 1.49844110\n",
      "Iteration 29, loss = 1.46074545\n",
      "Iteration 30, loss = 1.42439560\n",
      "Iteration 31, loss = 1.36910342\n",
      "Iteration 32, loss = 1.34293061\n",
      "Iteration 33, loss = 1.29264813\n",
      "Iteration 34, loss = 1.25696379\n",
      "Iteration 35, loss = 1.21564092\n",
      "Iteration 36, loss = 1.18675797\n",
      "Iteration 37, loss = 1.15041184\n",
      "Iteration 38, loss = 1.10110755\n",
      "Iteration 39, loss = 1.06137417\n",
      "Iteration 40, loss = 1.01984397\n",
      "Iteration 41, loss = 0.99047430\n",
      "Iteration 42, loss = 0.96418662\n",
      "Iteration 43, loss = 0.91753763\n",
      "Iteration 44, loss = 0.88119186\n",
      "Iteration 45, loss = 0.84211372\n",
      "Iteration 46, loss = 0.81453738\n",
      "Iteration 47, loss = 0.78288258\n",
      "Iteration 48, loss = 0.74895536\n",
      "Iteration 49, loss = 0.71988141\n",
      "Iteration 50, loss = 0.69381601\n",
      "Iteration 51, loss = 0.66606780\n",
      "Iteration 52, loss = 0.62193958\n",
      "Iteration 53, loss = 0.59841107\n",
      "Iteration 54, loss = 0.57037668\n",
      "Iteration 55, loss = 0.54076498\n",
      "Iteration 56, loss = 0.51181562\n",
      "Iteration 57, loss = 0.48600329\n",
      "Iteration 58, loss = 0.46373220\n",
      "Iteration 59, loss = 0.44027969\n",
      "Iteration 60, loss = 0.41872404\n",
      "Iteration 61, loss = 0.39463726\n",
      "Iteration 62, loss = 0.37173007\n",
      "Iteration 63, loss = 0.35402348\n",
      "Iteration 64, loss = 0.33274072\n",
      "Iteration 65, loss = 0.30821558\n",
      "Iteration 66, loss = 0.29902192\n",
      "Iteration 67, loss = 0.28069080\n",
      "Iteration 68, loss = 0.26388181\n",
      "Iteration 69, loss = 0.25861810\n",
      "Iteration 70, loss = 0.22853391\n",
      "Iteration 71, loss = 0.21268452\n",
      "Iteration 72, loss = 0.20791114\n",
      "Iteration 73, loss = 0.18910051\n",
      "Iteration 74, loss = 0.17503550\n",
      "Iteration 75, loss = 0.15980468\n",
      "Iteration 76, loss = 0.15298579\n",
      "Iteration 77, loss = 0.13703413\n",
      "Iteration 78, loss = 0.12516736\n",
      "Iteration 79, loss = 0.11677126\n",
      "Iteration 80, loss = 0.11415154\n",
      "Iteration 81, loss = 0.10229108\n",
      "Iteration 82, loss = 0.09645968\n",
      "Iteration 83, loss = 0.08729475\n",
      "Iteration 84, loss = 0.08197559\n",
      "Iteration 85, loss = 0.07660734\n",
      "Iteration 86, loss = 0.06752880\n",
      "Iteration 87, loss = 0.06253822\n",
      "Iteration 88, loss = 0.05890403\n",
      "Iteration 89, loss = 0.05390964\n",
      "Iteration 90, loss = 0.05079048\n",
      "Iteration 91, loss = 0.04670859\n",
      "Iteration 92, loss = 0.04430986\n",
      "Iteration 93, loss = 0.04019120\n",
      "Iteration 94, loss = 0.03767187\n",
      "Iteration 95, loss = 0.03550351\n",
      "Iteration 96, loss = 0.03277304\n",
      "Iteration 97, loss = 0.03081218\n",
      "Iteration 98, loss = 0.02916816\n",
      "Iteration 99, loss = 0.02715159\n",
      "Iteration 100, loss = 0.02600851\n",
      "Iteration 101, loss = 0.02392378\n",
      "Iteration 102, loss = 0.02270001\n",
      "Iteration 103, loss = 0.02126325\n",
      "Iteration 104, loss = 0.02029040\n",
      "Iteration 105, loss = 0.01913276\n",
      "Iteration 106, loss = 0.01829492\n",
      "Iteration 107, loss = 0.01749025\n",
      "Iteration 108, loss = 0.01652940\n",
      "Iteration 109, loss = 0.01556304\n",
      "Iteration 110, loss = 0.01487953\n",
      "Iteration 111, loss = 0.01427732\n",
      "Iteration 112, loss = 0.01354757\n",
      "Iteration 113, loss = 0.01281665\n",
      "Iteration 114, loss = 0.01249275\n",
      "Iteration 115, loss = 0.01175937\n",
      "Iteration 116, loss = 0.01121419\n",
      "Iteration 117, loss = 0.01080030\n",
      "Iteration 118, loss = 0.01033326\n",
      "Iteration 119, loss = 0.00989211\n",
      "Iteration 120, loss = 0.00942928\n",
      "Iteration 121, loss = 0.00913784\n",
      "Iteration 122, loss = 0.00869466\n",
      "Iteration 123, loss = 0.00832125\n",
      "Iteration 124, loss = 0.00813738\n",
      "Iteration 125, loss = 0.00791829\n",
      "Iteration 126, loss = 0.00763269\n",
      "Iteration 127, loss = 0.00723203\n",
      "Iteration 128, loss = 0.00692196\n",
      "Iteration 129, loss = 0.00665658\n",
      "Iteration 130, loss = 0.00642248\n",
      "Iteration 131, loss = 0.00622262\n",
      "Iteration 132, loss = 0.00602882\n",
      "Iteration 133, loss = 0.00583741\n",
      "Iteration 134, loss = 0.00565319\n",
      "Iteration 135, loss = 0.00548726\n",
      "Iteration 136, loss = 0.00534611\n",
      "Iteration 137, loss = 0.00516606\n",
      "Iteration 138, loss = 0.00501870\n",
      "Iteration 139, loss = 0.00483533\n",
      "Iteration 140, loss = 0.00467967\n",
      "Iteration 141, loss = 0.00455443\n",
      "Iteration 142, loss = 0.00448724\n",
      "Iteration 143, loss = 0.00436529\n",
      "Iteration 144, loss = 0.00429408\n",
      "Iteration 145, loss = 0.00412122\n",
      "Iteration 146, loss = 0.00401275\n",
      "Iteration 147, loss = 0.00390635\n",
      "Iteration 148, loss = 0.00380371\n",
      "Iteration 149, loss = 0.00371395\n",
      "Iteration 150, loss = 0.00363149\n",
      "Training size was  0\n",
      "Confusion Matrix for the above model is .....\n",
      "[[ 1  0  0 ...  0  1  0]\n",
      " [ 0  0  2 ...  1  3  0]\n",
      " [ 1  1 21 ...  4  3 20]\n",
      " ...\n",
      " [ 1  1  2 ... 56  2  2]\n",
      " [ 2  1  2 ...  0 60  1]\n",
      " [ 1  0 23 ...  2  3 27]]\n",
      "\n",
      "Classwise Accuracies .....\n",
      "[0.04545455 0.         0.12068966 0.         0.17241379 0.16062176\n",
      " 0.12616822 0.19649123 0.16       0.16666667 0.22491349 0.\n",
      " 0.         0.         0.25083612 0.         0.18729097 0.21722846\n",
      " 0.25367647 0.20503597 0.         0.20700637 0.11180124 0.22307692\n",
      " 0.24193548 0.02380952 0.12217195 0.1416309  0.15508021 0.\n",
      " 0.         0.         0.         0.11042945 0.         0.23728814\n",
      " 0.20761246 0.13846154]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizan/opt/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.19214832\n",
      "Iteration 2, loss = 2.87015657\n",
      "Iteration 3, loss = 2.71422031\n",
      "Iteration 4, loss = 2.56712383\n",
      "Iteration 5, loss = 2.46000365\n",
      "Iteration 6, loss = 2.38761332\n",
      "Iteration 7, loss = 2.33427924\n",
      "Iteration 8, loss = 2.28407692\n",
      "Iteration 9, loss = 2.24285237\n",
      "Iteration 10, loss = 2.20406812\n",
      "Iteration 11, loss = 2.16368089\n",
      "Iteration 12, loss = 2.12723905\n",
      "Iteration 13, loss = 2.08988037\n",
      "Iteration 14, loss = 2.05641272\n",
      "Iteration 15, loss = 2.01739788\n",
      "Iteration 16, loss = 1.98432539\n",
      "Iteration 17, loss = 1.95183197\n",
      "Iteration 18, loss = 1.91292772\n",
      "Iteration 19, loss = 1.87191410\n",
      "Iteration 20, loss = 1.83583709\n",
      "Iteration 21, loss = 1.79541310\n",
      "Iteration 22, loss = 1.76174069\n",
      "Iteration 23, loss = 1.71951227\n",
      "Iteration 24, loss = 1.69425836\n",
      "Iteration 25, loss = 1.64849002\n",
      "Iteration 26, loss = 1.61038594\n",
      "Iteration 27, loss = 1.57037181\n",
      "Iteration 28, loss = 1.52637121\n",
      "Iteration 29, loss = 1.48833588\n",
      "Iteration 30, loss = 1.45448190\n",
      "Iteration 31, loss = 1.41447843\n",
      "Iteration 32, loss = 1.37388966\n",
      "Iteration 33, loss = 1.33453548\n",
      "Iteration 34, loss = 1.29607769\n",
      "Iteration 35, loss = 1.26251950\n",
      "Iteration 36, loss = 1.21828864\n",
      "Iteration 37, loss = 1.18503700\n",
      "Iteration 38, loss = 1.15242208\n",
      "Iteration 39, loss = 1.11895389\n",
      "Iteration 40, loss = 1.07965660\n",
      "Iteration 41, loss = 1.04212113\n",
      "Iteration 42, loss = 1.01199998\n",
      "Iteration 43, loss = 0.97577554\n",
      "Iteration 44, loss = 0.94348390\n",
      "Iteration 45, loss = 0.90675855\n",
      "Iteration 46, loss = 0.87862856\n",
      "Iteration 47, loss = 0.85605108\n",
      "Iteration 48, loss = 0.82032377\n",
      "Iteration 49, loss = 0.79684465\n",
      "Iteration 50, loss = 0.75429291\n",
      "Iteration 51, loss = 0.73099480\n",
      "Iteration 52, loss = 0.70410816\n",
      "Iteration 53, loss = 0.67465881\n",
      "Iteration 54, loss = 0.65897245\n",
      "Iteration 55, loss = 0.62973066\n",
      "Iteration 56, loss = 0.59875129\n",
      "Iteration 57, loss = 0.57164524\n",
      "Iteration 58, loss = 0.55371228\n",
      "Iteration 59, loss = 0.52764009\n",
      "Iteration 60, loss = 0.50368422\n",
      "Iteration 61, loss = 0.49092452\n",
      "Iteration 62, loss = 0.46729771\n",
      "Iteration 63, loss = 0.43991302\n",
      "Iteration 64, loss = 0.41359298\n",
      "Iteration 65, loss = 0.40450860\n",
      "Iteration 66, loss = 0.38422786\n",
      "Iteration 67, loss = 0.36169014\n",
      "Iteration 68, loss = 0.33954518\n",
      "Iteration 69, loss = 0.32171724\n",
      "Iteration 70, loss = 0.30684126\n",
      "Iteration 71, loss = 0.28779309\n",
      "Iteration 72, loss = 0.27144736\n",
      "Iteration 73, loss = 0.25770799\n",
      "Iteration 74, loss = 0.24960534\n",
      "Iteration 75, loss = 0.23214140\n",
      "Iteration 76, loss = 0.22004912\n",
      "Iteration 77, loss = 0.20920097\n",
      "Iteration 78, loss = 0.19832609\n",
      "Iteration 79, loss = 0.17975473\n",
      "Iteration 80, loss = 0.17211890\n",
      "Iteration 81, loss = 0.15862363\n",
      "Iteration 82, loss = 0.14736552\n",
      "Iteration 83, loss = 0.13957064\n",
      "Iteration 84, loss = 0.13121579\n",
      "Iteration 85, loss = 0.11855297\n",
      "Iteration 86, loss = 0.11537073\n",
      "Iteration 87, loss = 0.10405707\n",
      "Iteration 88, loss = 0.09572697\n",
      "Iteration 89, loss = 0.08907533\n",
      "Iteration 90, loss = 0.08127369\n",
      "Iteration 91, loss = 0.07499342\n",
      "Iteration 92, loss = 0.06855243\n",
      "Iteration 93, loss = 0.06326816\n",
      "Iteration 94, loss = 0.05860230\n",
      "Iteration 95, loss = 0.05603715\n",
      "Iteration 96, loss = 0.05279768\n",
      "Iteration 97, loss = 0.04931738\n",
      "Iteration 98, loss = 0.04488488\n",
      "Iteration 99, loss = 0.04028400\n",
      "Iteration 100, loss = 0.03777784\n",
      "Iteration 101, loss = 0.03594384\n",
      "Iteration 102, loss = 0.03449235\n",
      "Iteration 103, loss = 0.04731044\n",
      "Iteration 104, loss = 0.43065424\n",
      "Iteration 105, loss = 0.35042431\n",
      "Iteration 106, loss = 0.14914524\n",
      "Iteration 107, loss = 0.06113419\n",
      "Iteration 108, loss = 0.03788886\n",
      "Iteration 109, loss = 0.02951540\n",
      "Iteration 110, loss = 0.02583372\n",
      "Iteration 111, loss = 0.02372005\n",
      "Iteration 112, loss = 0.02211018\n",
      "Iteration 113, loss = 0.02130173\n",
      "Iteration 114, loss = 0.01993166\n",
      "Iteration 115, loss = 0.01912709\n",
      "Iteration 116, loss = 0.01850954\n",
      "Iteration 117, loss = 0.01752352\n",
      "Iteration 118, loss = 0.01680454\n",
      "Iteration 119, loss = 0.01611840\n",
      "Iteration 120, loss = 0.01579544\n",
      "Iteration 121, loss = 0.01504150\n",
      "Iteration 122, loss = 0.01458439\n",
      "Iteration 123, loss = 0.01425792\n",
      "Iteration 124, loss = 0.01347628\n",
      "Iteration 125, loss = 0.01304310\n",
      "Iteration 126, loss = 0.01261255\n",
      "Iteration 127, loss = 0.01232181\n",
      "Iteration 128, loss = 0.01183708\n",
      "Iteration 129, loss = 0.01139385\n",
      "Iteration 130, loss = 0.01111019\n",
      "Iteration 131, loss = 0.01085982\n",
      "Iteration 132, loss = 0.01041902\n",
      "Iteration 133, loss = 0.01005080\n",
      "Iteration 134, loss = 0.00989923\n",
      "Iteration 135, loss = 0.00950869\n",
      "Iteration 136, loss = 0.00914825\n",
      "Iteration 137, loss = 0.00892729\n",
      "Iteration 138, loss = 0.00869073\n",
      "Iteration 139, loss = 0.00832570\n",
      "Iteration 140, loss = 0.00812050\n",
      "Iteration 141, loss = 0.00785062\n",
      "Iteration 142, loss = 0.00766265\n",
      "Iteration 143, loss = 0.00740331\n",
      "Iteration 144, loss = 0.00720637\n",
      "Iteration 145, loss = 0.00705438\n",
      "Iteration 146, loss = 0.00679712\n",
      "Iteration 147, loss = 0.00657711\n",
      "Iteration 148, loss = 0.00639396\n",
      "Iteration 149, loss = 0.00625697\n",
      "Iteration 150, loss = 0.00607781\n",
      "Training size was  -20000\n",
      "Confusion Matrix for the above model is .....\n",
      "[[ 1  0  0 ...  0  1  0]\n",
      " [ 0  0  1 ...  1  0  1]\n",
      " [ 1  1 20 ...  2  1 24]\n",
      " ...\n",
      " [ 0  0  0 ... 36  2  2]\n",
      " [ 1  2  4 ...  1 35  2]\n",
      " [ 1  0 19 ...  1  1 27]]\n",
      "\n",
      "Classwise Accuracies .....\n",
      "[0.05       0.         0.13333333 0.         0.14285714 0.21367521\n",
      " 0.136      0.20833333 0.17054264 0.15079365 0.23255814 0.\n",
      " 0.         0.05263158 0.22051282 0.         0.17010309 0.23857868\n",
      " 0.23863636 0.2        0.         0.21904762 0.10526316 0.25903614\n",
      " 0.26380368 0.05       0.125      0.1835443  0.13669065 0.\n",
      " 0.         0.         0.05263158 0.10909091 0.         0.2384106\n",
      " 0.18918919 0.17647059]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizan/opt/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.14907750\n",
      "Iteration 2, loss = 2.81508821\n",
      "Iteration 3, loss = 2.63587664\n",
      "Iteration 4, loss = 2.51234877\n",
      "Iteration 5, loss = 2.42202395\n",
      "Iteration 6, loss = 2.36106000\n",
      "Iteration 7, loss = 2.31117443\n",
      "Iteration 8, loss = 2.27241854\n",
      "Iteration 9, loss = 2.23966628\n",
      "Iteration 10, loss = 2.19863599\n",
      "Iteration 11, loss = 2.16953098\n",
      "Iteration 12, loss = 2.14061293\n",
      "Iteration 13, loss = 2.10040976\n",
      "Iteration 14, loss = 2.06386062\n",
      "Iteration 15, loss = 2.03560797\n",
      "Iteration 16, loss = 2.00061300\n",
      "Iteration 17, loss = 1.95942220\n",
      "Iteration 18, loss = 1.92620610\n",
      "Iteration 19, loss = 1.89333415\n",
      "Iteration 20, loss = 1.85834881\n",
      "Iteration 21, loss = 1.81780852\n",
      "Iteration 22, loss = 1.77686930\n",
      "Iteration 23, loss = 1.73491578\n",
      "Iteration 24, loss = 1.69716589\n",
      "Iteration 25, loss = 1.66144789\n",
      "Iteration 26, loss = 1.62058896\n",
      "Iteration 27, loss = 1.58163178\n",
      "Iteration 28, loss = 1.54719835\n",
      "Iteration 29, loss = 1.50723764\n",
      "Iteration 30, loss = 1.47983748\n",
      "Iteration 31, loss = 1.43080734\n",
      "Iteration 32, loss = 1.38901114\n",
      "Iteration 33, loss = 1.35565481\n",
      "Iteration 34, loss = 1.31720386\n",
      "Iteration 35, loss = 1.28116340\n",
      "Iteration 36, loss = 1.24153160\n",
      "Iteration 37, loss = 1.20442950\n",
      "Iteration 38, loss = 1.16911394\n",
      "Iteration 39, loss = 1.13951447\n",
      "Iteration 40, loss = 1.09954086\n",
      "Iteration 41, loss = 1.06844635\n",
      "Iteration 42, loss = 1.04055030\n",
      "Iteration 43, loss = 1.00655299\n",
      "Iteration 44, loss = 0.96414470\n",
      "Iteration 45, loss = 0.93495951\n",
      "Iteration 46, loss = 0.90726615\n",
      "Iteration 47, loss = 0.87330578\n",
      "Iteration 48, loss = 0.84345910\n",
      "Iteration 49, loss = 0.80743582\n",
      "Iteration 50, loss = 0.78332828\n",
      "Iteration 51, loss = 0.76015545\n",
      "Iteration 52, loss = 0.73046593\n",
      "Iteration 53, loss = 0.69874279\n",
      "Iteration 54, loss = 0.67438361\n",
      "Iteration 55, loss = 0.64354825\n",
      "Iteration 56, loss = 0.62136169\n",
      "Iteration 57, loss = 0.59825207\n",
      "Iteration 58, loss = 0.56959308\n",
      "Iteration 59, loss = 0.54289465\n",
      "Iteration 60, loss = 0.52391118\n",
      "Iteration 61, loss = 0.50007953\n",
      "Iteration 62, loss = 0.47660180\n",
      "Iteration 63, loss = 0.44757482\n",
      "Iteration 64, loss = 0.42959741\n",
      "Iteration 65, loss = 0.41012776\n",
      "Iteration 66, loss = 0.38882102\n",
      "Iteration 67, loss = 0.36479916\n",
      "Iteration 68, loss = 0.34790927\n",
      "Iteration 69, loss = 0.33004359\n",
      "Iteration 70, loss = 0.31696357\n",
      "Iteration 71, loss = 0.29290157\n",
      "Iteration 72, loss = 0.27596521\n",
      "Iteration 73, loss = 0.26017679\n",
      "Iteration 74, loss = 0.24787152\n",
      "Iteration 75, loss = 0.23495384\n",
      "Iteration 76, loss = 0.22065052\n",
      "Iteration 77, loss = 0.20763976\n",
      "Iteration 78, loss = 0.19553418\n",
      "Iteration 79, loss = 0.18088522\n",
      "Iteration 80, loss = 0.16766157\n",
      "Iteration 81, loss = 0.16114539\n",
      "Iteration 82, loss = 0.14300881\n",
      "Iteration 83, loss = 0.13669713\n",
      "Iteration 84, loss = 0.13202836\n",
      "Iteration 85, loss = 0.12787779\n",
      "Iteration 86, loss = 0.11177265\n",
      "Iteration 87, loss = 0.10271397\n",
      "Iteration 88, loss = 0.09233145\n",
      "Iteration 89, loss = 0.09137581\n",
      "Iteration 90, loss = 0.08335132\n",
      "Iteration 91, loss = 0.07239869\n",
      "Iteration 92, loss = 0.06770105\n",
      "Iteration 93, loss = 0.06293838\n",
      "Iteration 94, loss = 0.05897223\n",
      "Iteration 95, loss = 0.05233779\n",
      "Iteration 96, loss = 0.04710404\n",
      "Iteration 97, loss = 0.04368943\n",
      "Iteration 98, loss = 0.04110777\n",
      "Iteration 99, loss = 0.03741030\n",
      "Iteration 100, loss = 0.03491753\n",
      "Iteration 101, loss = 0.03472860\n",
      "Iteration 102, loss = 0.03133917\n",
      "Iteration 103, loss = 0.02886751\n",
      "Iteration 104, loss = 0.02800268\n",
      "Iteration 105, loss = 0.02567286\n",
      "Iteration 106, loss = 0.02479970\n",
      "Iteration 107, loss = 0.02245153\n",
      "Iteration 108, loss = 0.02080399\n",
      "Iteration 109, loss = 0.02003988\n",
      "Iteration 110, loss = 0.01850005\n",
      "Iteration 111, loss = 0.01733537\n",
      "Iteration 112, loss = 0.01609578\n",
      "Iteration 113, loss = 0.01528912\n",
      "Iteration 114, loss = 0.01544027\n",
      "Iteration 115, loss = 0.01404779\n",
      "Iteration 116, loss = 0.01320946\n",
      "Iteration 117, loss = 0.01229465\n",
      "Iteration 118, loss = 0.01164409\n",
      "Iteration 119, loss = 0.01120760\n",
      "Iteration 120, loss = 0.01083747\n",
      "Iteration 121, loss = 0.01019083\n",
      "Iteration 122, loss = 0.00985925\n",
      "Iteration 123, loss = 0.00923518\n",
      "Iteration 124, loss = 0.00904398\n",
      "Iteration 125, loss = 0.00879558\n",
      "Iteration 126, loss = 0.00820498\n",
      "Iteration 127, loss = 0.00791765\n",
      "Iteration 128, loss = 0.00754932\n",
      "Iteration 129, loss = 0.00721070\n",
      "Iteration 130, loss = 0.00689549\n",
      "Iteration 131, loss = 0.00663657\n",
      "Iteration 132, loss = 0.00630713\n",
      "Iteration 133, loss = 0.00612718\n",
      "Iteration 134, loss = 0.00600237\n",
      "Iteration 135, loss = 0.00593775\n",
      "Iteration 136, loss = 0.00563057\n",
      "Iteration 137, loss = 0.00538228\n",
      "Iteration 138, loss = 0.00523699\n",
      "Iteration 139, loss = 0.00505242\n",
      "Iteration 140, loss = 0.00488829\n",
      "Iteration 141, loss = 0.00481899\n",
      "Iteration 142, loss = 0.00457941\n",
      "Iteration 143, loss = 0.00452621\n",
      "Iteration 144, loss = 0.00434751\n",
      "Iteration 145, loss = 0.00426368\n",
      "Iteration 146, loss = 0.00413344\n",
      "Iteration 147, loss = 0.00405702\n",
      "Iteration 148, loss = 0.00396236\n",
      "Iteration 149, loss = 0.00388346\n",
      "Iteration 150, loss = 0.00381221\n",
      "Training size was  -40000\n",
      "Confusion Matrix for the above model is .....\n",
      "[[ 1  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  1  1]\n",
      " [ 0  1  9 ...  0  2  9]\n",
      " ...\n",
      " [ 0  0  0 ... 29  0  1]\n",
      " [ 0  1  0 ...  0 21  0]\n",
      " [ 0  0  8 ...  0  2  9]]\n",
      "\n",
      "Classwise Accuracies .....\n",
      "[0.11111111 0.         0.13636364 0.         0.10909091 0.24590164\n",
      " 0.11594203 0.18947368 0.08333333 0.1969697  0.20869565 0.\n",
      " 0.         0.         0.22352941 0.         0.19148936 0.18181818\n",
      " 0.24489796 0.19101124 0.         0.27956989 0.05454545 0.31578947\n",
      " 0.2804878  0.         0.15189873 0.07936508 0.15517241 0.\n",
      " 0.         0.         0.         0.09836066 0.         0.31182796\n",
      " 0.25609756 0.14285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizan/opt/anaconda3/envs/ai/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "Y = pandas.DataFrame(dataField[\"Suggested Job Role\"])\n",
    "X = pandas.DataFrame(dataField.drop(\"Suggested Job Role\", 1, inplace=False))\n",
    "\n",
    "Y= Y.astype(numpy.float32)\n",
    "X = X.astype(numpy.float32)\n",
    "confusion_sets = [0.4,0.3,0.2,0.1]\n",
    "for i in range(len(confusion_sets)):\n",
    "    train1, test1, train2, test2 = train_test_split(X, Y, test_size = confusion_sets[i], random_state = 10)\n",
    "    model = MLPClassifier(solver='adam', hidden_layer_sizes=(700, 200), max_iter=150,\n",
    "                        verbose=2, activation='relu', random_state=5)\n",
    "\n",
    "    model.fit(train1, train2.to_numpy().ravel())\n",
    "\n",
    "    y_pred = model.predict(test1)\n",
    "    conf_m = confusion_matrix(y_pred, test2)\n",
    "    print(\"Training size was \", (1 - confusion_sets[i])*20000)\n",
    "    print(\"Confusion Matrix for the above model is .....\")\n",
    "    print(conf_m) #Confusion Matrix\n",
    "    print()\n",
    "    print(\"Classwise Accuracies .....\")\n",
    "    print(conf_m.diagonal()/conf_m.sum(axis=1)) #Class-wise Accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('ai': conda)",
   "name": "python3913jvsc74a57bd06f2d7cd1279468eec8e51b904316773d989c33d473d8a700d2587dd963d54f9b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "6f2d7cd1279468eec8e51b904316773d989c33d473d8a700d2587dd963d54f9b"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}